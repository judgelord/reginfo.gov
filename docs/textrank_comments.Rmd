---
title: "CFPB Payday Loan Rule"
subtitle: "Comments summarized with `textrank` compared to hand-coded key sentences"
output:
    # pdf_document:
    #   toc: true
    #   keep_tex: true
    html_document:
      highlight: zenburn
      toc: true
      toc_float: true
      code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r global.options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = FALSE, 
                      fig.width=8.5, 
                      split = T,
                      fig.align = 'center', 
                      fig.path='figs/',
                      warning=FALSE, 
                      message=FALSE)


library(tidyverse)
library(magrittr)
library(tidytext)
library(xml2)
library(knitr)
library(kableExtra)

library(ggplot2); theme_set(theme_bw())
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., direction = -1)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., direction = -1)
  
  
kablebox <- . %>%  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "500px")
```

 This file summarizes individual comments for agency rules. To do this it uses three functions; `clean_comments()`, `summarizeText()`, and `summarize_comments()`.  
 
 `summarize_comments()` takes in four inputs `text`, `n_sentence`s, `max_sentences`, `custom_stop_words`, and `agency_name`  
*n_sentences* is the number of sentences to use for the summary  
function maps `summarizeText()` onto each each comment and then extracts the top *n_sentences*

 `summarizeText()` takes in four inputs `text`, `max_sentences`, `custom_stop_word`s, and `agency_name`  
  *text* is the string of text for each comment  
  *max_sentences* is a numerical value for the max number of sentences textrank will pull in and summarize  
  *custom_stop_words* is a vector of words to be removed from the dictionary of words in textrank  
  *agency_name* name of the agency to be removed from textrank as a string
function removes custom_stop_words, agency_name, and stop_words from textrank and then uses textrank to rank the sentences by importance.
  
  `clean_comments()` takes in a string of text and cleans it. It is run on each comment provided.  
  

You can download it [here](https://github.com/judgelord/rulemaking/blob/master/functions/textrank.R) or load it in R with `source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")`

For more on textrank and the steps involved in applying it to a rule, see https://judgelord.github.io/rulemaking/textrank_summary and the PDF that Chris shared.


```{r}
# load summarize_comment() function
source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")

# load text cleaning function 
source(here::here("functions", "clean_string.R"))
```


To assess the quality of computer-generated summaries, we can compare them to the key sentences identified by hand. Specifically, RAs were asked to identify the top three key "asks" in each comment and select the text associated with those askes in each comment. They also identified the commenter's organization, overall position on the proposed rule, and several other key things that might help assess the quality of textrank-generated summaries. 

Load hand-coded data.

```{r gs}
load(here::here("data", "CFPB-2016-0025-coded.Rdata"))

coded %<>% mutate(hand_selected_summary = paste(ask, ask1, ask2, ask3, sep = "...") %>% str_remove_all("\\...NA")) 
```


Get file paths comment texts.

```{r data}
# file paths for comments 
agency <- "CFPB"
docket <- "CFPB-2016-0025"

# A dataframe of custom stop words, formatted like tidytext::stop_words
# If form letters need to be summariezed standard headings should be removed (docket no., email, etc.)
custom_stop_words <- tibble( 
  words = c("cfpb", "payday, vehicle title, and certain high-cost installment loans")) %>% # ignore case?

  mutate(words = words)%>%
  unnest_tokens(output = word, input = words) %>%
  distinct()

agency_name <- c("consumer financial protection bureau")

# get txt file names from a directory, here called “comment_text”
comments <- tibble( path = list.files( here::here('comment_text', agency, docket), 
                                       full.names = T) ) %>% 
  filter( str_detect(path, "txt") )

# in SQL, CFPB file names are regs_dot_gov_document_id, shortened to document_id for now
comments %<>% 
  mutate( document_id = path %>%
            str_remove(".*/")  %>%
            str_remove("-[0-9]*\\..*") 
   ) 


d <- comments %>% 
  # select comments that have been hand coded
  filter(document_id %in% coded$document_id)

#d %<>% head() #FIXME just working with a few comments for now
```

Clean comment texts.

```{r summarize}
# remove some common things. This can be generalized with info from data

# These should go in custom_stop_words, not here; this is valid text.
# Only needed if we are capturing summaries for form letters
# comment_header <- c("Docket No. CFP Consumer Financial Protection Bureau G Street, NW. Washington, DC")
# 
# comment_footer <- c("Sincerely,|Signature|Print Name|Address|City|State|Docket No\\. CFP|Zip|Email|Date")
# 
# agency_address <- c("Dear Consumer Financial Protection Bureau CFPB")

clean_comments <- . %>% 
  str_c(collapse = " ") %>% 
  # remove tab and line breaks
  str_replace_all("\\t|\\\n", " ") %>% 
  str_replace_all("U\\.S\\.", "United States") %>% #Issues with cut of sentences due to period
  #str_remove_all(" ?(f|ht)(tp)(s?)(://)([A-z]*)[.|/](.*)") %>%
  #str_replace_all("\\s+", " ") %>% # removed by str_squish()
  #str_remove_all("^[0-9](?!\\.)") %>% #FIXME are not numbers removed below
  # add missing spaces after periods
  str_replace_all("\\.([A-z])", ". \\1") %>%
  # remvove numbers and specials (keep only text and basic punctuation)
  str_replace_all("[^[A-z] \\.\\,\\?\\!\\;&\\;\\']", " ") %>% 
  str_remove_all("\\[|\\]") %>%
  str_remove_all("§") %>%
  #str_remove_all("\\'") %>%
  str_squish() %>%
  #str_replace_all(" (\\.|\\?|\\!||:|;)", "\\1 ") %>%
  str_replace_all(" , ", " ") %>%
  str_replace_all("_", " ") %>%
  # double commas  
  #str_replace_all("\\, \\, ", ", ") %>% 
  # double periods 
  #str_replace_all("\\. \\. ", ". ") %>% 
  # one character after a period 
  str_replace_all("\\. \\. \\. ", ". ") %>% 
  # remove white space
  str_squish() %>% 
  str_remove_all("pagebreak") %>% # Devin's OCR method adds this 
  # remove repeated periods
  #str_replace_all("\\.*", ". ") %>% #removed below
  # str_replace_all(" \\,", ", ") %>%
  str_replace_all(" \\.", ". ") %>%
  #remove space in 's
  str_replace_all(" \\'s ", "\\'s ") %>%
  #str_replace_all(" '", "'") %>% # we want to keep "'"
  # remove web addresses, wont capture urls with punctuation ("." or "_") in the middle
  # str_remove_all("www\\.[A-z]*\\.(com|org|net|gov|pdf)") %>%  # does not currently impact textrank, uncomment if urls start to skew summary
  # str_remove_all("http:www\\.[A-z]*\\.(com|org|net|gov|pdf)") %>%
  # str_remove_all("files\\.[A-z]*\\.(com|org|net|gov|pdf)") %>%
  #Removes duplicated puncuation
  str_replace_all("([[:punct:]])\\1+", "\\1") %>%
  str_squish() 

read_comments <- . %>%
  read_lines() %>%
  clean_comments() 

# map read
d$text <- map_chr(d$path, read_comments)
```


```{r testing, include = FALSE, eval = FALSE}
# inspect
d$text[10]

d$text %>% kablebox()

#Missing files
coded %>%
  filter(document_id %in% coded$document_id) %>%
  filter(document_id %in% d$document_id == FALSE) %>%
  select(document_id, comment_url) %>%
  kablebox()
  
```


Summarize comments.

```{r}
# for testing 
#text <- d$text[4]
 #max_sentences = 100

summarizeText <- function(text, max_sentences, custom_stop_words, agency_name) {

  sentences <- tibble(text = text) %>%
    unnest_sentences(output = sentences, input = text) %>%
    distinct() %>%
    mutate(textrank_id = row_number()) %>% 
    # textrank requires columns in order
    select(textrank_id, sentences) 
  
  # remove agency name string from textrank dictionary
  sentences %<>%
    mutate(sentences = str_remove_all(sentences, agency_name))
   
  # select max sentences to summarize per section
  sentences %<>% filter(textrank_id <= max_sentences)
  
  
  # textrank needs a dictionary of words
  words <- unnest_tokens(sentences, output = word, input = 'sentences') %>% 
    distinct() %>% 
    anti_join(tidytext::stop_words) %>% 
    anti_join(custom_stop_words) # remove custom stop words from textrank
  
  # inspect 
  count(words, word, sort = T) %>% filter(n>2) %>% pull(word)
  

     
  # textRank fails if you feed it only one sentence
  if(nrow(sentences) > 1){
    out <- textrank::textrank_sentences(data = sentences,
                                        terminology = words)
    
    
    # arrange by textrank 
    out$sentences %<>% arrange(-textrank)
    
    # so we format the unranked sentences data frame as an alternative
  } else {
    out <- list(sentences = as.data.frame(sentences))
  }
  
  #out %>%   knitr::kable() 
  
  return(out)
  
}


summarize_comments <- function(text, n_sentences = 2, max_sentences = 100, custom_stop_words, agency_name = "") {
  

  # not cleaning data here either; should we? I think it is better to do it in the above function so it is done on one comment at a time.

  # summarize, map summary function to each unique section
  text_summary <-  map(.x = text,
                       .f = summarizeText,
                       max_sentences = max_sentences,
                       custom_stop_words = custom_stop_words,
                       agency_name = agency_name) 
  
  # extract top n_sentences
  pull_sentences <- . %>% 
    .$sentences %>% 
    pull(sentence) %>% 
    .[1:n_sentences] %>% 
    str_to_sentence() %>%  #FIXME should define a custom function to fix sentence case with some common fixes like:
    str_replace_all(" i ", " I ") %>% 
    str_replace_all(" u\\.s\\. ", " U.S. ") %>% 
    str_replace_all("nprm", "NPRM") %>% 
    str_c(collapse = " ")
  
  # add n_sentences pulled from textrank output to data
  summary <- text_summary %>%
      #FIXME when this fails, it should default to the first sentence of the section. 
      map_chr(possibly(pull_sentences, otherwise = " ") ) 
  
  return(summary)
}



comment_summary <- summarize_comments(d$text, 
                                      n_sentences = 3, 
                                      max_sentences = 50, 
                                      custom_stop_words = custom_stop_words,
                                      agency_name = agency_name)
```



Compare to hand-coded key sentences.

```{r}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% kablebox()
```

### Observations about Textrank

- Comments tend to fall into two categories  
  1. Opposes the rule in its entirety
  2. Supports an amended form of the rule
- Textrank tends to capture factual statements over firm stances  
  - For example of opposition to the rule see McIntyre & Lemon, PLLC & The Independent Finance Association of Illinois  
  - For example of changes to a rule see NCRC & The Clearing House Association LLC
- The `textrank_summary` tends to explain why an organization is in support or in opposition.  
- Generated summaries do a fairly good job of capturing changes an organization would like to see to the rule.   
- The hand coded summary contains the ask whereas the `textrank_summary` tends to contain sentences that explain that ask.  

**Examples:**

```{r obs}
 d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == c("McIntyre & Lemon, PLLC", "The Independent Finance Association of Illinois", "Community Spirit Bank")) %>%
  kablebox()

```





### Agency Name

- `summarizeText` removes agency name from Textrank.
- Typically the difference between `textrank_summaries` with and without the agency name is one sentences.  
- When agency name is removed this tends to run the risk of capturing headings.   
- Each option has their pros and cons and each method produces some good summaries and some bad summaries.

**Examples:**
The italizied sentences are the changes between the summaries when agency name is removed and when agency name is kept.

**Organization: The Chippewa Cree Tribe of the Rocky Boys Reservation**

With Agency:
Once again, nowhere in the NPRM does the bureau acknowledge tribal regulation in the small dollar lending industry. There is no discussion whatsoever of tribal regulation of small dollar lending, an omission that undercuts any empirical foundation for the proposed rule and significantly and negatively impacts the rulemaking analysis. *Because of all these deficiencies, the bureau should rescind the proposed rule entirely.*

Without Agency:
Once again, nowhere in the NPRM does the bureau acknowledge tribal regulation in the small dollar lending industry. *Jackson on behalf of the chippewa cree tribe of the rocky boy s reservation tribe I submit this public comment to express the tribe s views of the proposed regulations from the cfpb or bureau regarding small dollar lending proposed rule.* There is no discussion whatsoever of tribal regulation of small dollar lending, an omission that undercuts any empirical foundation for the proposed rule and significantly and negatively impacts the rulemaking analysis.


**Organization: Ballard Spahr**

With Agency Name:  
The bureau ' s own analysis shows that its proposed rule w ill have a devastating impact both on consumers who rely on covered loans and on providers of covered loans. *The proposed rule was published by the consumer financial protection bureau cfpb or ' bureau in fed.* I the proposed rule.

Without Agency Name:  
The bureau ’ s own analysis shows that its proposed rule w ill have a devastating impact both on consumers who rely on covered loans and on providers of covered loans. I the proposed rule. *Monica jackson october page executive summary consumers rely upon loans subject to the proposed rule covered loans to pay for emergency expenses, to avoid late charges and nsf fees, and to add ress other serious financia l needs.*


**Organization: Katten Muchin Rosenman LLP**

With Agency Name:  
*Jackson the comments set forth below are provided in connection with the payday, vehicle title and certain high cost installment loans proposal set forth by the consumer financial protection bureau cfpb or bureau in the federal register on july the proposal.* Like many providers included within the scope of the proposal, our client appreciates this opportunity to comment on the first regulatory exercise of the cfpb's statutory authority to deter unfair and abusive acts and practices in connection with the provision of consumer financial products and services. Dfa o.


Without Agency Name:  
Like many providers included within the scope of the proposal, our client appreciates this opportunity to comment on the first regulatory exercise of the cfpb’s statutory authority to deter unfair and abusive acts and practices in connection with the provision of consumer financial products and services. *See dfa l c l. Dfa I a.*

### Textrank Limitations  
- Repeated headers or footings can be captured by textrank.  
- Does not produce good summaries for form/campaign letters.

- - -

### Dealing with Bad OCRs



```{r investigate}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == "Ballard Spahr") %>%
  kablebox()


```

**Textrank is capturing split words as important. Once this is fixed in the loading of text files this could change the summaries with more words being captured by textrank**
```{r spliceSentences}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == "The Independent Finance Association of Illinois") %>%
  kablebox()
  
```


**Textrank is capturing state names & 'textrank_summary' has output from footnote that is not captured by textrank**
```{r stateNames}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == "State Attorney Generals") %>%
  kablebox()

```




### Notes

Things to consider:Will want to replace modified sentences with complete sentences:  
* Will still want to run `clean_comments()` on these sentences  
* May want to drop short sentences (sentences under 7 words). Headers get caught by textrank when there is more than one page for a comment.  
* U.S. had to be replaced with United States in order to prevent splicing. Other common acronyms may cause issues in the same way.
