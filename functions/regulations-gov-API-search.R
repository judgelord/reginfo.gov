# This script builds functions to pull from the regulations.gov API, see http://regulationsgov.github.io/developers/console/#!/documents.json/documents_get_0

# grab your regulations.gov API key from a seperate file
# source("api-key.R") 
# or otherwise define it
# api_key <- api_key

library(httr)
library(jsonlite)
library(tidyverse)
library(magrittr)


# defaults 
url  <- "https://api.data.gov"
rpp <- 1000 # results per page
order <- "DESC" # DESC: Decending, ASC: Ascending 
sortby <- "postedDate" #docketId (Docket ID) docId (Document ID) title (Title) postedDate (Posted Date) agency (Agency) documentType (Document Type) submitterName (Submitter Name) organization (Organization)
page <- c(0, seq(1000)*rpp) # up to 1,000,0000 results
status <- "O" # O for open docket
n <- 2000 # max number of results
start <- 1000 # result number on which to resume partial search (e.g. due to api limits )
documenttype <- "N%2BPR%2BFR%2BPS%2BSR%2BO"
## N: Notice, 
## PR: Proposed Rule, 
## FR: Rule, 
## O: Other, 
## SR: Supporting & Related Material, 
## PS: Public Submission


#####################################
# ALL DOCUMENTS (i.e. not a keyword search, not limited to certian agencies) #
#####################################
search.docs <- function(documenttype, n) {
  documenttype <- gsub(", ", "%2B", documenttype)
  
  # create path
  path <- paste0("/regulations/v3/documents?api_key=", api_key, 
                 "&rpp=", rpp, 
                 "&so=", order, 
                 "&sb=", sortby, 
                 "&dct=", documenttype)
  
  path1 <- paste0(path, "&po=", page[round(start/1000)]) # page 1
  
  ######################
  # for > 1000 results #
  ######################
  # initialize
  raw.result <- GET(url = url, path = path1)
  raw.result$status_code == 200
  content <- fromJSON(rawToChar(raw.result$content))
  all <- as.data.frame(content[[1]])
  # loop over and bind additional pages 
  if(n>=2000){
  for(i in round(start/1000)+1:round(n/1000)){
    Sys.sleep(60)
    #tryCatch({
    raw.result <- GET(url = url, path = paste0(path, "&po=", page[i]))
    if(raw.result$status_code == 200){
      content <- fromJSON(rawToChar(raw.result$content))
      temp <- as.data.frame(content[[1]])
      all <- rbind(all, temp)
    }
    #}, error = function(err) {print(i)})
  }
  }
  all <- unique(all)
  return(all)
  if(raw.result$status_code != 200){ print(paste("Error: status code =", raw.result$status_code) ) }
}
##########################################################
##########################################################




########################################
# For select agencies # 
#######################
search.agency.docs <- function(agency, documenttype, n) {
  documenttype <- gsub(", ", "%2B", documenttype)
  agency <- gsub(", ", "%2B", agency)


# create path
path <- paste0("/regulations/v3/documents?api_key=", api_key, 
               "&rpp=", rpp, 
               "&a=", agency,
               "&so=", order, 
               "&sb=", sortby, 
               "&dct=", documenttype)

path1 <- paste0(path, "&po=", page[round(start/1000)]) # page 1

######################
# for > 1000 results #
######################
# initialize
raw.result <- GET(url = url, path = path1)
raw.result$status_code == 200
content <- fromJSON(rawToChar(raw.result$content))
all <- as.data.frame(content[[1]])
# loop over and bind additional pages 
if(n>=2000){
for(i in round(start/1000)+1:round(n/1000)){
  #tryCatch({
  raw.result <- GET(url = url, path = paste0(path, "&po=", page[i]))
  if(raw.result$status_code == 200){
    content <- fromJSON(rawToChar(raw.result$content))
    temp <- as.data.frame(content[[1]])
    all <- rbind(all, temp)
  }
  #}, error = function(err) {print(i)})
}
}
all <- unique(all)
return(all)
if(raw.result$status_code != 200){ print(paste("Error: status code =", raw.result$status_code) ) }
}




###########
# docket #
##########
search.docket <- function(docket, documenttype, n){
path <- paste0("/regulations/v3/documents?api_key=", api_key, 
               "&rpp=", rpp, 
               "&D=", docket,
               "&so=", order, 
               "&sb=", sortby, 
               "&dct=", documenttype)
path1 <- paste0(path, "&po=", page[round(start/1000)]) # page 1
raw.result <- GET(url = url, path = path1)
raw.result$status_code == 200
content <- fromJSON(rawToChar(raw.result$content))
all <- as.data.frame(content[[1]])
# loop over and bind additional pages 
if(n>=2000){
  for(i in round(start/1000)+1:round(n/1000)){
    #tryCatch({
    raw.result <- GET(url = url, path = paste0(path, "&po=", page[i]))
    if(raw.result$status_code == 200){
      content <- fromJSON(rawToChar(raw.result$content))
      temp <- as.data.frame(content[[1]])
      all <- rbind(all, temp)
    }
    #}, error = function(err) {print(i)})
  }
}
all <- unique(all)
return(all)
if(raw.result$status_code != 200){ print(paste("Error: status code =", raw.result$status_code) ) }
}
######################################
############################################################################








########################################################
# KEYWORD SEARCH #
##################


######################
# function for each keyword path 
######################
search.keyword <- function(path) {
  # initialize with page 1
  rpp <- 1000
  page <- c(0, seq(1000)*rpp)
  raw.result <- GET(url = url, path = paste0(path, "&po=", page[round(start/1000)]))
  content <- fromJSON(rawToChar(raw.result$content))
  d <- as.data.frame(content[[1]])
  # loop over and bind additional pages 
  for(i in round(start/1000)+1:round(n/1000)){ # up to 10k results. MAY NEED TO CHANGE THIS BUT KEPT LOW TO AVOID LIMIT
    # tryCatch({
    if(raw.result$status_code == 200){ # only try pull if previous call was good
      raw.result <- GET(url = url, path = paste0(path, "&po=", page[i]))
    }
    if(raw.result$status_code == 200){ # only rbind if new call is good
      content <- fromJSON(rawToChar(raw.result$content))
      dtemp <- as.data.frame(content[[1]])
      if(ncol(dtemp)==ncol(d)){d <- rbind(d, dtemp)}
    }
    # }, error = function(err) {print(i)})  # may not need tryCatch because rbind throughs an error
  } # end loop
  return(d)
} # end keyword function


# create path function for keyword searches, comment out things like agency or status
search.keywords <- function(documenttype, keywords, n) {
  keywords <- str_c("&22", keywords, "&22") %>% str_replace(" ", "%2B")
  documenttype <- gsub(", ", "%2B", documenttype)
  
  regs.gov.path <- function(documenttype, search){
    paste0("/regulations/v3/documents?api_key=", api_key, 
           "&rpp=", rpp, 
           #"&a=", agency,
           "&so=", order, 
           "&sb=", sortby, 
           "&s=", search, 
           #"&cp=", status,
           "&dct=", documenttype)
  }

# clear paths
paths <- NA

# make a vector of paths for each keyword
for(i in 1:length(keywords)){
  paths[i] <- regs.gov.path(
    documenttype = documenttype, # "N%2BPR%2BFR%2BPS%2BSR%2BO" # N%2BPR%2BFR%2BPS%2BSR%2BO = all docs, N: Notice PR: Proposed Rule FR: Rule O: Other SR: Supporting & Related Material PS: Public Submission
    search = keywords[i])
}


###################
# get keyword 1  #
##################
d <- search.keyword(paths[1])

# rename search results column
names(d)[names(d) == 'summary'] <- gsub("%22|%2B", "", keywords[1])

###################################
# repeat and merge any additional keywords #
###################################
if(length(keywords) > 1){
for(i in 2:length(paths)){
  temp <- search.keywords(paths[i])
  names(temp)[names(temp) == 'summary'] <- gsub("%22|%2B", " ", keywords[i])
  d <- full_join(d, temp)
}
}
# clean up strings 
# FIXME
# docs %<>% dplyr::mutate_all(str_replace(., pattern = "\n|\t|,", replacement =" "))

return(d)
} # end search.keywords() function 


search_keyword_page <- function(page, documenttype, keyword){
  
  # format (put in quotes and replace space with unicode)
search <- str_c("%22", keyword, "%22") %>% str_replace(" ", "%2B")
documenttype <- gsub(", ", "%2B", documenttype)
rpp <- 1000
page <- page*rpp
path <- paste0("/regulations/v3/documents?api_key=", api_key, 
         "&rpp=", rpp, 
         #"&a=", agency,
         "&so=", order, 
         "&sb=", sortby, 
         "&s=", search, 
         #"&cp=", status,
         "&dct=", documenttype,
         "&po=", page)
  
  
  raw.result <- GET(url = url, path = path)
  
  content <- fromJSON(rawToChar(raw.result$content))
  
  d <- as.data.frame(content[[1]]) %>% as_tibble() %>% 
    mutate(page = page/rpp)
  
  return(d)
}

# api v4
# https://api.regulations.gov/v4/comments?filter[searchTerm]=water&api_key=DEMO_KEY
# https://api.regulations.gov/v4/comments?filter[commentOnId]=09000064846eebaf
# &page[size]=250
# &page[number]=N
# &sort=lastModifiedDate,documentId
# &api_key=DEMO_KEY

# https://api.regulations.gov/v4/comments?filter[commentOnId]=09000064846eebaf
# &filter[lastModifiedDate][ge]=2020-08-10 11:58:52
# &page[size]=250
# &page[number]=N
# &sort=lastModifiedDate,documentId
# &api_key=DEMO_KEY


search_keyword_page4 <- function(page = 1, 
                                 documenttype = "Rule", 
                                 keyword, 
                                 lastModifiedDate = Sys.time() ){
  
  
  lastModifiedDate %<>% str_replace_all("[A-Z]", " ") %>%  str_squish()
  
  # format (replace space with unicode)
  search <- #keyword %>% 
    str_c("%22", keyword, "%22") %>% 
    str_replace(" ", "%2B")
  
  
  endpoint = ifelse(documenttype == "Public Submission", "comments", "documents")
  
  documentType = ifelse(documenttype == "Public Submission", "", "&filter[documentType]=documents")

  path <- paste0("/v4/", endpoint,
                 "?page[number]=", page,
                 "&page[size]=250", 
                 documentType,
                 #"&a=", agency,
                 "&sort=-lastModifiedDate,documentId",
                 "&filter[searchTerm]=", search,
                 "&filter[lastModifiedDate][le]=", lastModifiedDate,
                 "&api_key=", api_key)
  
  # this works:
  # raw.result <- GET(url = "https://api.regulations.gov", 
  # path = "/v4/comments?filter[searchTerm]=environmental%2Bjustice&api_key=aynn8SLo5zdb2V0wqBKQwHQ5FmCLd2cIpWStzrZ0")
  
  str_c("https://api.regulations.gov", path)
  
  raw.result <- GET(url = "https://api.regulations.gov", path = path)
  
  content <- fromJSON(rawToChar(raw.result$content))
  
  d <- content$data$attributes %>%  as_tibble()  %>%
    mutate(id = content$data$id,
           type = content$data$type,
           links = content$data$links$self,
      lastpage = content$meta$lastPage)
  
  #TODO loop this over batches of 5k documents
  # if(content$meta$lastPage){
  #   lastModifiedDate <-- content$data$attributes$lastModifiedDate %>% tail(1)
  #   #lastModifiedDate <-- Sys.time() %>% str_remove(" [A-Z]")
  # } 
  
  return(d)
}

d <- search_keyword_page4(keyword = "environmental justice",
                          documenttype = "Public Submission",
                          lastModifiedDate =  "2020-05-15 18:39:57")
d$lastModifiedDate
d$highlightedContent

# d %<>% filter(
#   !is.na(commentText)|
#     !is.na(summary)|
#     !is.na(court)|
#     #!is.na(lawsuit)|
#     !is.na(sue)|
#     !is.na(appeal)  
# )

# # remove tabs and line breaks 
# d$summary <- gsub("\n|\t|,", "", d$summary)
# d$environmentaljustice <- gsub("\n|\t|,", "", d$environmentaljustice)
# d$commentText <- gsub("\n|\t|,", "", d$commentText)





###############################
# One document (requires knowing the document id)
# docID <- d$documentId[2]

search.doc <- function(docID) {


  # create path (NOTE: for one document, the path is ...v3/document?..., not ...v3/documents?..)
  # This path is for an API search to get the file formats
  # To download these files, see regulations-gov-get-attachments.R, which uses the content streamer, not the API
  path <- paste0("/regulations/v3/document?api_key=", api_key, 
                 "&documentId=", docID)

  raw.result <- GET(url = url, path = path)
  raw.result$status_code == 200
  content <- fromJSON(rawToChar(raw.result$content))
  doc <- tibble(documentId = docID, 
                attach.url = str_c( unlist(content$attachments$fileFormats), collapse = ";")
                )
  
  if(raw.result$status_code != 200){ print(paste("Error: status code =", raw.result$status_code) ) }
  
  # sleep to avoid 429 overload errors when applied to many documents
  Sys.sleep(3)
  
  return(doc)
  }
# end get attachment loop












##########################################################


# # Beginner example 
# # path to first page of up to 1000 results for first search term
# path1 <- paste0(path[1], "&po=", page[round(start/1000)]) # page 1
# 
# 
# # Execute API call for up to 1000 results from first term 
# raw.result <- GET(url = url, path = path1)
# raw.result$status_code == 200 # did it work? 
# 
# content <- fromJSON(rawToChar(raw.result$content))
# class(content) # make sure it's a list
# length(content) # of 2


# d <- as.data.frame(content[[1]])
# 
# d$summary <- gsub("\n", "", d$summary)
# d$commentText <- gsub("\n", "", d$commentText)
# write.csv(names(d), paste(documenttype, keywords, ".csv"))